#!/usr/bin/env python3
"""
Script d'installation et configuration Ollama pour FinAgent.

Ce script automatise l'installation d'Ollama et le t√©l√©chargement des mod√®les recommand√©s.
"""

import asyncio
import os
import sys
import platform
import subprocess
import time
from pathlib import Path
from typing import List, Optional

import requests
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn
from rich.panel import Panel
from rich.table import Table

# Ajouter le r√©pertoire parent au PYTHONPATH
sys.path.insert(0, str(Path(__file__).parent.parent))

from finagent.ai.providers.ollama_provider import create_ollama_provider
from finagent.ai.models.base import ModelType

console = Console()


class OllamaInstaller:
    """Installateur automatique pour Ollama."""
    
    def __init__(self):
        self.console = console
        self.system = platform.system().lower()
        self.architecture = platform.machine().lower()
        
        # Mod√®les recommand√©s par taille
        self.recommended_models = {
            "small": [
                ModelType.GEMMA_2B,
                ModelType.PHI3_MINI,
            ],
            "medium": [
                ModelType.LLAMA3_1_8B,
                ModelType.MISTRAL_7B,
                ModelType.GEMMA_7B,
                ModelType.CODELLAMA_7B,
            ],
            "large": [
                ModelType.LLAMA3_1_70B,
                ModelType.CODELLAMA_13B,
                ModelType.MISTRAL_22B,
            ]
        }
    
    def check_system_requirements(self) -> bool:
        """V√©rifie les pr√©requis syst√®me."""
        console.print("[cyan]V√©rification des pr√©requis syst√®me...[/cyan]")
        
        # V√©rification de l'OS
        if self.system not in ["linux", "darwin", "windows"]:
            console.print(f"[red]‚ùå Syst√®me d'exploitation non support√©: {self.system}[/red]")
            return False
        
        console.print(f"[green]‚úÖ Syst√®me: {platform.system()} {platform.release()}[/green]")
        
        # V√©rification de l'architecture
        if "arm" in self.architecture or "aarch64" in self.architecture:
            console.print(f"[green]‚úÖ Architecture: {self.architecture} (optimis√© pour ARM)[/green]")
        elif "x86_64" in self.architecture or "amd64" in self.architecture:
            console.print(f"[green]‚úÖ Architecture: {self.architecture}[/green]")
        else:
            console.print(f"[yellow]‚ö†Ô∏è Architecture: {self.architecture} (non test√©e)[/yellow]")
        
        # V√©rification de la RAM
        try:
            import psutil
            memory_gb = psutil.virtual_memory().total / (1024**3)
            console.print(f"[green]‚úÖ RAM: {memory_gb:.1f} GB[/green]")
            
            if memory_gb < 4:
                console.print("[yellow]‚ö†Ô∏è RAM faible - recommand√©: au moins 8GB pour de bonnes performances[/yellow]")
            elif memory_gb < 8:
                console.print("[yellow]üí° Pour les gros mod√®les (70B+), 16GB+ de RAM recommand√©s[/yellow]")
        except ImportError:
            console.print("[yellow]‚ö†Ô∏è Impossible de v√©rifier la RAM (psutil non install√©)[/yellow]")
        
        return True
    
    def is_ollama_installed(self) -> bool:
        """V√©rifie si Ollama est d√©j√† install√©."""
        try:
            result = subprocess.run(["ollama", "--version"], 
                                  capture_output=True, text=True, timeout=10)
            if result.returncode == 0:
                version = result.stdout.strip()
                console.print(f"[green]‚úÖ Ollama d√©j√† install√©: {version}[/green]")
                return True
        except (subprocess.TimeoutExpired, FileNotFoundError):
            pass
        
        console.print("[yellow]üì¶ Ollama non trouv√© - installation requise[/yellow]")
        return False
    
    def install_ollama(self) -> bool:
        """Installe Ollama selon le syst√®me d'exploitation."""
        console.print("[cyan]Installation d'Ollama...[/cyan]")
        
        try:
            if self.system == "linux" or self.system == "darwin":
                # Installation via script curl pour Linux/macOS
                console.print("T√©l√©chargement et ex√©cution du script d'installation...")
                
                # T√©l√©charge le script d'installation
                response = requests.get("https://ollama.ai/install.sh", timeout=30)
                response.raise_for_status()
                
                # Ex√©cute le script
                process = subprocess.Popen(
                    ["sh"],
                    stdin=subprocess.PIPE,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True
                )
                
                stdout, stderr = process.communicate(input=response.text)
                
                if process.returncode == 0:
                    console.print("[green]‚úÖ Ollama install√© avec succ√®s[/green]")
                    return True
                else:
                    console.print(f"[red]‚ùå Erreur d'installation: {stderr}[/red]")
                    return False
            
            elif self.system == "windows":
                console.print("[yellow]Installation manuelle requise pour Windows:[/yellow]")
                console.print("1. Visitez https://ollama.ai/download")
                console.print("2. T√©l√©chargez l'installateur Windows")
                console.print("3. Ex√©cutez l'installateur")
                console.print("4. Red√©marrez ce script apr√®s l'installation")
                return False
            
        except Exception as e:
            console.print(f"[red]‚ùå Erreur lors de l'installation: {e}[/red]")
            return False
        
        return False
    
    def start_ollama_service(self) -> bool:
        """D√©marre le service Ollama."""
        console.print("[cyan]D√©marrage du service Ollama...[/cyan]")
        
        try:
            # V√©rifie si le service est d√©j√† en cours
            result = subprocess.run(
                ["curl", "-s", "http://localhost:11434/api/version"],
                capture_output=True, timeout=5
            )
            
            if result.returncode == 0:
                console.print("[green]‚úÖ Service Ollama d√©j√† en cours[/green]")
                return True
            
        except (subprocess.TimeoutExpired, FileNotFoundError):
            pass
        
        try:
            # D√©marre Ollama en arri√®re-plan
            if self.system == "windows":
                subprocess.Popen(["ollama", "serve"], creationflags=subprocess.CREATE_NEW_CONSOLE)
            else:
                subprocess.Popen(["ollama", "serve"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            
            # Attend que le service soit pr√™t
            console.print("Attente du d√©marrage du service...")
            for i in range(30):  # 30 secondes max
                try:
                    result = subprocess.run(
                        ["curl", "-s", "http://localhost:11434/api/version"],
                        capture_output=True, timeout=2
                    )
                    if result.returncode == 0:
                        console.print("[green]‚úÖ Service Ollama d√©marr√©[/green]")
                        return True
                except:
                    pass
                
                time.sleep(1)
            
            console.print("[red]‚ùå Timeout - impossible de d√©marrer le service[/red]")
            return False
            
        except Exception as e:
            console.print(f"[red]‚ùå Erreur lors du d√©marrage: {e}[/red]")
            return False
    
    async def test_ollama_connection(self) -> bool:
        """Teste la connexion √† Ollama."""
        console.print("[cyan]Test de connexion √† Ollama...[/cyan]")
        
        try:
            provider = create_ollama_provider()
            connected = await provider.validate_connection()
            
            if connected:
                console.print("[green]‚úÖ Connexion Ollama r√©ussie[/green]")
                
                # Affiche les mod√®les d√©j√† install√©s
                models = await provider.get_available_models_info()
                if models:
                    console.print(f"[blue]üì¶ {len(models)} mod√®le(s) d√©j√† install√©(s)[/blue]")
                    for model in models[:3]:  # Affiche les 3 premiers
                        console.print(f"   ‚Ä¢ {model.name} ({model.size_gb:.1f}GB)")
                    if len(models) > 3:
                        console.print(f"   ‚Ä¢ ... et {len(models) - 3} autre(s)")
                
                return True
            else:
                console.print("[red]‚ùå Impossible de se connecter √† Ollama[/red]")
                return False
                
        except Exception as e:
            console.print(f"[red]‚ùå Erreur de connexion: {e}[/red]")
            return False
    
    def select_models_to_install(self) -> List[ModelType]:
        """Interface interactive pour s√©lectionner les mod√®les √† installer."""
        console.print("\n[cyan]S√©lection des mod√®les √† installer[/cyan]")
        
        # Affiche les options de taille
        table = Table(title="Cat√©gories de mod√®les")
        table.add_column("Taille", style="cyan")
        table.add_column("Mod√®les", style="white")
        table.add_column("RAM requise", style="yellow")
        table.add_column("Description", style="dim")
        
        table.add_row(
            "Small", 
            "Gemma 2B, Phi3 Mini", 
            "4-6 GB",
            "Rapides, bonne qualit√© g√©n√©rale"
        )
        table.add_row(
            "Medium", 
            "Llama 3.1 8B, Mistral 7B", 
            "8-12 GB",
            "√âquilibre performance/qualit√©"
        )
        table.add_row(
            "Large", 
            "Llama 3.1 70B, CodeLlama 13B", 
            "16+ GB",
            "Haute qualit√©, plus lents"
        )
        
        console.print(table)
        
        while True:
            choice = console.input("\n[yellow]Choisissez une cat√©gorie (small/medium/large/custom/skip): [/yellow]").lower().strip()
            
            if choice == "skip":
                return []
            elif choice in ["small", "medium", "large"]:
                selected_models = self.recommended_models[choice]
                console.print(f"[green]S√©lection: {len(selected_models)} mod√®les de taille {choice}[/green]")
                return selected_models
            elif choice == "custom":
                return self._select_custom_models()
            else:
                console.print("[red]Choix invalide. Utilisez: small, medium, large, custom, ou skip[/red]")
    
    def _select_custom_models(self) -> List[ModelType]:
        """S√©lection personnalis√©e de mod√®les."""
        console.print("\n[cyan]S√©lection personnalis√©e de mod√®les[/cyan]")
        
        all_models = []
        for category_models in self.recommended_models.values():
            all_models.extend(category_models)
        
        # Supprime les doublons
        unique_models = list(set(all_models))
        unique_models.sort(key=lambda x: x.value)
        
        console.print("Mod√®les disponibles:")
        for i, model in enumerate(unique_models, 1):
            console.print(f"  {i:2d}. {model.value}")
        
        selected = []
        while True:
            choice = console.input("\n[yellow]Entrez les num√©ros des mod√®les (ex: 1,3,5) ou 'done': [/yellow]").strip()
            
            if choice.lower() == "done":
                break
            
            try:
                indices = [int(x.strip()) - 1 for x in choice.split(",")]
                for idx in indices:
                    if 0 <= idx < len(unique_models):
                        model = unique_models[idx]
                        if model not in selected:
                            selected.append(model)
                            console.print(f"[green]‚úÖ Ajout√©: {model.value}[/green]")
                        else:
                            console.print(f"[yellow]‚ö†Ô∏è D√©j√† s√©lectionn√©: {model.value}[/yellow]")
                    else:
                        console.print(f"[red]‚ùå Index invalide: {idx + 1}[/red]")
            except ValueError:
                console.print("[red]‚ùå Format invalide. Utilisez des num√©ros s√©par√©s par des virgules.[/red]")
        
        return selected
    
    async def install_models(self, models: List[ModelType]) -> bool:
        """Installe les mod√®les s√©lectionn√©s."""
        if not models:
            console.print("[yellow]Aucun mod√®le √† installer[/yellow]")
            return True
        
        console.print(f"\n[cyan]Installation de {len(models)} mod√®le(s)...[/cyan]")
        
        provider = create_ollama_provider()
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TaskProgressColumn(),
            console=console
        ) as progress:
            
            overall_task = progress.add_task("Installation globale", total=len(models))
            
            for i, model in enumerate(models, 1):
                model_task = progress.add_task(f"Installation {model.value}", total=100)
                
                try:
                    console.print(f"\n[blue]üì• Installation de {model.value} ({i}/{len(models)})[/blue]")
                    
                    # Simule le progr√®s (Ollama ne fournit pas de callback de progr√®s)
                    for j in range(0, 101, 10):
                        progress.update(model_task, completed=j)
                        await asyncio.sleep(0.1)
                    
                    success = await provider.pull_model(model.value)
                    
                    if success:
                        progress.update(model_task, completed=100)
                        progress.update(overall_task, advance=1)
                        console.print(f"[green]‚úÖ {model.value} install√© avec succ√®s[/green]")
                    else:
                        console.print(f"[red]‚ùå √âchec de l'installation de {model.value}[/red]")
                        
                except Exception as e:
                    console.print(f"[red]‚ùå Erreur lors de l'installation de {model.value}: {e}[/red]")
        
        console.print("\n[green]üéâ Installation des mod√®les termin√©e[/green]")
        return True
    
    async def run_setup(self):
        """Ex√©cute le processus complet d'installation."""
        console.print(Panel.fit("ü§ñ Configuration Ollama pour FinAgent", style="cyan bold"))
        
        # 1. V√©rification des pr√©requis
        if not self.check_system_requirements():
            return False
        
        # 2. V√©rification/Installation d'Ollama
        if not self.is_ollama_installed():
            if not self.install_ollama():
                return False
        
        # 3. D√©marrage du service
        if not self.start_ollama_service():
            return False
        
        # 4. Test de connexion
        if not await self.test_ollama_connection():
            return False
        
        # 5. S√©lection et installation des mod√®les
        models_to_install = self.select_models_to_install()
        if models_to_install:
            await self.install_models(models_to_install)
        
        # 6. Configuration finale
        console.print("\n[green]üéâ Configuration Ollama termin√©e avec succ√®s![/green]")
        console.print("\n[cyan]Prochaines √©tapes:[/cyan]")
        console.print("1. [white]Configurez vos variables d'environnement dans .env[/white]")
        console.print("2. [white]Testez la configuration: finagent ai status[/white]")
        console.print("3. [white]Analysez une action: finagent analyze AAPL --provider ollama[/white]")
        
        return True


async def main():
    """Point d'entr√©e principal."""
    try:
        installer = OllamaInstaller()
        success = await installer.run_setup()
        sys.exit(0 if success else 1)
        
    except KeyboardInterrupt:
        console.print("\n[yellow]üëã Installation interrompue par l'utilisateur[/yellow]")
        sys.exit(1)
    except Exception as e:
        console.print(f"\n[red]üí• Erreur inattendue: {e}[/red]")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())